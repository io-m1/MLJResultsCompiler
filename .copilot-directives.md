# MLJ Results Compiler - GitHub Copilot Automation Directives

**Last Updated**: January 31, 2026  
**Status**: Backend implementation in progress  
**Deployment Target**: Render.com or Railway.app  

---

## ðŸš¨ CRITICAL CONTEXT

**THIS APPLICATION IS BROKEN:**
- Frontend deployed on Vercel (working UI)
- Backend DOES NOT EXIST
- No API endpoints
- No file processing
- No database
- Users upload files that go nowhere

**THIS DOCUMENT PROVIDES EXACT INSTRUCTIONS FOR GITHUB COPILOT TO FIX EVERYTHING**

---

## PROJECT OVERVIEW

### What We're Building
A professional test result compilation system that:
- Accepts Excel file uploads (TEST_1.xlsx through TEST_5.xlsx)
- Validates Excel format and structure
- Merges data from all test files
- Calculates scores using specific formula
- Generates output Excel file
- Provides download and history tracking

### Architecture Decision
- **Frontend**: Next.js on Vercel (already built)
- **Backend**: Node.js + Express (MUST CREATE)
- **Database**: PostgreSQL (MUST CREATE)
- **Storage**: Local filesystem or AWS S3
- **Deployment**: Render.com (free tier available)

**WHY NOT PYTHON?** Python cannot run on Vercel. Must use Node.js backend on separate infrastructure.

---

## COPILOT PROMPT TEMPLATES

### Prompt 1: Create Express Backend Server

**Use this exact prompt in GitHub Copilot:**

```
Create a Node.js Express server with the following structure:

1. Main server file (src/server.js) that:
   - Initializes Express app
   - Enables CORS for https://mljresultscompiler.vercel.app
   - Sets up middleware for JSON parsing
   - Sets up middleware for file uploads (multer)
   - Listens on process.env.PORT (default 3000)

2. Create these API routes:
   - POST /api/upload: Accept multiple xlsx files
   - POST /api/process: Process uploaded files
   - GET /api/history: Return processing history
   - GET /api/download/:jobId: Stream result file

3. Add error handling middleware that:
   - Catches all errors
   - Returns JSON with error message
   - Logs errors to console
   - Returns appropriate HTTP status codes

4. Create a simple health check:
   - GET /health: Returns {status: "ok"}

Store files temporarily in a 'uploads' directory.
Use UUID for unique file IDs.
```

---

### Prompt 2: Create File Validation Functions

**Use this exact prompt:**

```
Create validation functions for Excel file processing in src/utils/validators.js:

1. validateExcelFile(filePath):
   - Check if file has .xlsx extension
   - Try reading file with xlsx library
   - Verify file isn't corrupted
   - Return {isValid: true} or {isValid: false, error: "specific message"}

2. validateExcelStructure(sheetData):
   - Check if data has columns: "Full Names", "Email", "Result"
   - Check if data has at least 1 row
   - Return {isValid: true, columnIndices} or {isValid: false, error: "message"}

3. validateBatchStructure(fileArray):
   - Ensure exactly 5 files provided (TEST_1 through TEST_5)
   - All files must pass validateExcelFile
   - All files must have same structure
   - Return {isValid: true, fileNames} or {isValid: false, error: "message"}

Error messages should be user-friendly:
- "File must be Excel format (.xlsx)"
- "File corrupted, cannot read data"
- "Missing required columns: Full Names, Email, Result"
- "Need exactly 5 test files"
- etc.

Include console.log for debugging each validation step.
```

---

### Prompt 3: Create Excel Processing Logic

**Use this exact prompt:**

```
Create Excel processing functions in src/utils/excelProcessor.js:

1. readExcelFile(filePath):
   - Read Excel file using 'xlsx' library
   - Extract sheet named "Responses" (or first sheet)
   - Convert to array of objects with headers
   - Return array of {Full Names, Email, Result, ...otherColumns}

2. createMergeKey(fullName):
   - Input: "John Doe"
   - Convert to lowercase
   - Trim whitespace
   - Remove extra spaces between names
   - Return: "john doe"
   - Used to merge same person across tests

3. mergeTestResults(test1Data, test2Data, test3Data, test4Data, test5Data):
   - For each person, create merge key from Full Names
   - Merge across all 5 test results using key
   - Keep all columns: Full Names, Email, TEST_1 Result, TEST_2 Result, etc.
   - For missing data, use empty string
   - Return array of merged records

4. calculateScores(mergedData):
   - For each record, calculate:
     - TEST_1, TEST_2, TEST_3, TEST_4, TEST_5 as numbers
     - Sum all test scores: TOTAL = SUM(all tests)
     - Calculate SCORE: SCORE = (TOTAL + 0.8) Ã— 16.6666
     - Assign STATUS: if SCORE >= 50 then "PASS" else "FAIL"
   - Add to record: {Full Names, Email, TEST_1, TEST_2, TEST_3, TEST_4, TEST_5, SCORE, STATUS, ...}
   - Return updated array

5. generateResultFile(processedData, outputPath):
   - Create new Excel workbook
   - Add sheet "Results"
   - Set column headers: S/N, Full Names, Email, TEST_1, TEST_2, TEST_3, TEST_4, TEST_5, SCORE, STATUS
   - Write all data rows
   - Format headers as bold
   - Auto-fit column widths
   - Save to outputPath
   - Return outputPath

6. processCompleteJob(filePathArray):
   - Input: array of 5 file paths [test1.xlsx, test2.xlsx, ...]
   - Call readExcelFile for each
   - Call mergeTestResults with all data
   - Call calculateScores on merged data
   - Call generateResultFile to create output
   - Return {success: true, resultFile, participantCount} or {success: false, error: "message"}

Include error handling and console logs at each step.
```

---

### Prompt 4: Create Database Schema

**Use this exact prompt:**

```
Create PostgreSQL database setup in src/db/schema.sql:

Create two tables:

1. processing_jobs:
   - id UUID PRIMARY KEY DEFAULT gen_random_uuid()
   - uploaded_at TIMESTAMP DEFAULT now()
   - processed_at TIMESTAMP NULL
   - status VARCHAR(20) DEFAULT 'uploading' (values: uploading, processing, complete, error)
   - input_files JSONB (array of uploaded filenames)
   - output_file_path TEXT NULL (path to result file)
   - result_file_url TEXT NULL (download URL)
   - error_message TEXT NULL
   - participant_count INT NULL
   - results_summary JSONB NULL {passCount: 10, failCount: 5}
   - created_by VARCHAR(255) NULL
   - metadata JSONB NULL

2. uploaded_files:
   - id UUID PRIMARY KEY DEFAULT gen_random_uuid()
   - job_id UUID FOREIGN KEY references processing_jobs(id)
   - original_filename VARCHAR(255)
   - stored_filename VARCHAR(255)
   - file_size INT
   - mime_type VARCHAR(100)
   - uploaded_at TIMESTAMP DEFAULT now()
   - status VARCHAR(20) DEFAULT 'uploaded'
   - validated BOOLEAN DEFAULT false
   - validation_errors JSONB NULL

Create indexes:
- ON processing_jobs(status)
- ON processing_jobs(uploaded_at)
- ON uploaded_files(job_id)

Include:
- Timestamps with timezone awareness
- Proper constraints and relationships
- Comments explaining each column
- Sample INSERT and SELECT queries
```

---

### Prompt 5: Create Database Functions

**Use this exact prompt:**

```
Create database operations in src/db/queries.js using pg library:

1. initializeDatabase():
   - Read schema.sql
   - Execute all SQL statements
   - Log "Database initialized"
   - Handle errors gracefully

2. saveJob(jobData):
   - Insert into processing_jobs
   - jobData: {inputFiles, status: 'uploading'}
   - Return jobId UUID

3. updateJobStatus(jobId, status, data):
   - Update processing_jobs SET status=?, ...
   - data can include: {processedAt, outputPath, errorMessage, participantCount}
   - Return updated record

4. saveUploadedFile(jobId, fileName, filePath, fileSize):
   - Insert into uploaded_files
   - Link to job
   - Return fileId

5. getJobHistory(limit = 50):
   - SELECT from processing_jobs
   - Order by uploaded_at DESC
   - Limit results
   - Include upload filenames from uploaded_files
   - Return array of jobs with details

6. getJobById(jobId):
   - SELECT from processing_jobs where id = jobId
   - Include all file details
   - Return complete job information

7. markJobComplete(jobId, resultFilePath, participantCount, passCount, failCount):
   - Update job status to 'complete'
   - Set processedAt to now()
   - Set output_file_path
   - Set participant_count
   - Set results_summary JSON
   - Return updated job

8. deleteOldFiles(daysOld = 7):
   - Delete files older than X days from uploads directory
   - Delete corresponding database records
   - Log what was deleted

All functions should:
- Use prepared statements (no SQL injection!)
- Return promises (async/await)
- Include error handling
- Log operations for debugging
- Handle NULL values properly
```

---

### Prompt 6: Create API Route Handlers

**Use this exact prompt:**

```
Create API route handlers in src/routes/

1. POST /api/upload (src/routes/upload.js):
   - Accept multipart/form-data with files
   - Extract files from request
   - For each file:
     - Validate Excel format using validateExcelFile()
     - Generate unique ID
     - Save to uploads directory
     - Save metadata to database
   - Return: {jobId, uploadedCount, totalSize, status: "ready_for_processing"}
   - On error: Return error with specific message

2. POST /api/process (src/routes/process.js):
   - Accept JSON: {jobId}
   - Validate jobId exists in database
   - Get all files for job
   - Validate batch structure
   - Call processCompleteJob(filePaths)
   - Save result file path to database
   - Update job status to 'complete'
   - Return: {jobId, status: "complete", resultFile, participantCount, passCount, failCount}
   - On error: Update job status to 'error' and return error message

3. GET /api/download/:jobId (src/routes/download.js):
   - Validate jobId exists
   - Check that job is complete
   - Get result file path
   - Verify file exists on disk
   - Stream file to user with headers:
     - Content-Type: application/vnd.openxmlformats-officedocument.spreadsheetml.sheet
     - Content-Disposition: attachment; filename="Results_[date].xlsx"
   - On error: Return 404 or appropriate error

4. GET /api/history (src/routes/history.js):
   - Get all processing jobs (last 50)
   - For each job, include:
     - Job ID, dates, status
     - Input file names
     - Participant count
     - Pass/fail counts
   - Order by most recent first
   - Return: {jobs: [...], totalCount}
   - On error: Return empty array with error message

Each route should:
- Validate input
- Log requests
- Handle errors gracefully
- Return JSON responses
- Return appropriate HTTP status codes (200, 400, 404, 500)
```

---

### Prompt 7: Create CORS and Security Middleware

**Use this exact prompt:**

```
Create middleware in src/middleware/

1. cors.js - Enable CORS for Vercel frontend:
   - Allow origin: https://mljresultscompiler.vercel.app
   - Allow origin: http://localhost:3000 (for development)
   - Allow methods: GET, POST, OPTIONS
   - Allow headers: Content-Type, Authorization
   - Allow credentials: true
   - Return headers properly

2. errorHandler.js - Global error handling:
   - Catch all errors
   - Log error details
   - Return JSON error response
   - Include error message (user-friendly)
   - Include error code if available
   - Don't expose internal details
   - Return 500 status for unhandled errors

3. requestLogger.js - Log all requests:
   - Log method, URL, timestamp
   - Log request size
   - Log response status and size
   - Log processing time
   - Use console.log for now

4. fileCleanup.js - Auto-delete old files:
   - Run every 6 hours
   - Delete uploaded files older than 7 days
   - Delete from database
   - Log what was deleted
   - Don't delete if job is still in progress
```

---

### Prompt 8: Create Environment Configuration

**Use this exact prompt:**

```
Create configuration files:

1. .env.example (template for all variables):
   NODE_ENV=production
   PORT=3000
   DATABASE_URL=postgresql://user:password@host:5432/dbname
   CORS_ORIGIN=https://mljresultscompiler.vercel.app
   MAX_FILE_SIZE=10485760
   UPLOAD_DIR=./uploads
   JWT_SECRET=your-secret-key-here
   LOG_LEVEL=info

2. config/config.js (load and validate environment):
   - Load from .env using dotenv
   - Validate required variables exist
   - Throw error if missing
   - Export configuration object
   - Include helpful error messages for missing vars

3. .env.development (for local development):
   DATABASE_URL=postgresql://localhost:5432/mlj_dev
   CORS_ORIGIN=http://localhost:3000
   LOG_LEVEL=debug

4. .env.production (Render.com template):
   NODE_ENV=production
   DATABASE_URL={will be set by Render}
   CORS_ORIGIN=https://mljresultscompiler.vercel.app
   PORT=3000
```

---

## STEP-BY-STEP IMPLEMENTATION GUIDE

### Step 1: Create Backend Project (15 minutes)
1. Create `backend/` folder âœ“ (already done)
2. Run `npm init -y` in backend folder
3. Install dependencies: `npm install express cors dotenv pg multer uuid xlsx`
4. Install dev dependencies: `npm install --save-dev nodemon`
5. Create `src/` folder structure âœ“ (already done)

### Step 2: Server & Routes (30 minutes)
Use Copilot Prompt 1 to create Express server
Use Copilot Prompt 6 to create route handlers

### Step 3: Validation (15 minutes)
Use Copilot Prompt 2 to create validators

### Step 4: Processing Logic (30 minutes)
Use Copilot Prompt 3 to create Excel processing

### Step 5: Database (20 minutes)
Use Copilot Prompt 4 to create schema
Use Copilot Prompt 5 to create queries

### Step 6: Middleware (10 minutes)
Use Copilot Prompt 7 to create middleware

### Step 7: Configuration (10 minutes)
Use Copilot Prompt 8 to create config files

### Step 8: Testing (30 minutes)
- Test each endpoint locally
- Test with real Excel files
- Check database integration
- Verify error handling

### Step 9: Deployment (20 minutes)
- Create Render.com account
- Connect GitHub repository
- Set environment variables
- Deploy backend

### Step 10: Frontend Integration (15 minutes)
Update frontend to call backend APIs

---

## CRITICAL FILES TO CREATE

```
backend/
â”œâ”€â”€ package.json                 (dependencies)
â”œâ”€â”€ .env.example                 (template)
â”œâ”€â”€ .gitignore                   (ignore uploads/, node_modules/, .env)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ server.js               (main Express app)
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ config.js           (configuration loading)
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ upload.js           (POST /api/upload)
â”‚   â”‚   â”œâ”€â”€ process.js          (POST /api/process)
â”‚   â”‚   â”œâ”€â”€ download.js         (GET /api/download/:jobId)
â”‚   â”‚   â””â”€â”€ history.js          (GET /api/history)
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ validators.js       (Excel validation)
â”‚   â”‚   â””â”€â”€ excelProcessor.js   (Excel reading/writing/merging)
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ schema.sql          (database tables)
â”‚   â”‚   â””â”€â”€ queries.js          (database operations)
â”‚   â””â”€â”€ middleware/
â”‚       â”œâ”€â”€ cors.js             (CORS setup)
â”‚       â”œâ”€â”€ errorHandler.js     (error handling)
â”‚       â”œâ”€â”€ requestLogger.js    (request logging)
â”‚       â””â”€â”€ fileCleanup.js      (old file cleanup)
â””â”€â”€ uploads/                     (temp file storage)
```

---

## DEPLOYMENT CHECKLIST

### Before Deploying to Render.com:
- [ ] All endpoints tested locally
- [ ] Database initialized and working
- [ ] File upload and download working
- [ ] Error messages are user-friendly
- [ ] Environment variables configured
- [ ] CORS set to Vercel domain
- [ ] No console.error or unhandled exceptions
- [ ] .gitignore includes uploads/, .env, node_modules/

### Render.com Deployment Steps:
1. Push code to GitHub
2. Create Render account (free)
3. Create new Web Service
4. Connect GitHub repository
5. Set environment variables:
   - DATABASE_URL: your PostgreSQL URL
   - CORS_ORIGIN: https://mljresultscompiler.vercel.app
   - NODE_ENV: production
6. Deploy
7. Test all endpoints
8. Get backend URL (e.g., https://mlj-backend.render.com)

### Frontend Update:
1. Add to vercel.json or frontend .env:
   ```
   NEXT_PUBLIC_BACKEND_URL=https://mlj-backend.render.com
   ```
2. Update API calls to use this URL
3. Redeploy frontend

---

## TESTING CHECKLIST

Test each scenario:

### Upload Endpoint
- [ ] Upload single file â†’ error "Need all 5 test files"
- [ ] Upload 5 valid files â†’ returns jobId
- [ ] Upload non-Excel file â†’ error "Must be .xlsx"
- [ ] Upload corrupted Excel â†’ error "Cannot read file"
- [ ] Upload files without required columns â†’ error with column names
- [ ] Upload large batch (10+ MB) â†’ succeeds
- [ ] Upload with special characters in filename â†’ works correctly

### Process Endpoint
- [ ] Process valid upload â†’ generates results
- [ ] Process non-existent jobId â†’ 404 error
- [ ] Process incomplete upload â†’ 400 error
- [ ] Processing completes in < 1 minute â†’ true
- [ ] Database updated with completion status â†’ verified
- [ ] Result file created and accessible â†’ verified

### Download Endpoint
- [ ] Download valid job result â†’ file downloads
- [ ] Download non-existent job â†’ 404 error
- [ ] Downloaded file opens in Excel â†’ verified
- [ ] File has all required columns â†’ verified
- [ ] Formulas are correct (not hardcoded) â†’ verified

### History Endpoint
- [ ] Returns list of all jobs â†’ verified
- [ ] Orders by most recent first â†’ verified
- [ ] Includes participant counts â†’ verified
- [ ] Includes pass/fail summaries â†’ verified
- [ ] Handles empty history â†’ returns []

### Error Handling
- [ ] Network errors handled gracefully â†’ verified
- [ ] Database connection error â†’ user-friendly message
- [ ] File system errors â†’ user-friendly message
- [ ] Invalid input data â†’ helpful error message
- [ ] Concurrent requests â†’ handled properly

---

## SUCCESS CRITERIA

Backend is working correctly when:

âœ… User can upload 5 Excel files  
âœ… Files are validated and saved  
âœ… User can click "Process" and see progress  
âœ… Processing completes without errors  
âœ… Results file is generated  
âœ… User can download results  
âœ… Downloaded file opens in Excel  
âœ… All columns present and correct  
âœ… Formulas calculate scores properly  
âœ… History shows all past jobs  
âœ… Can re-download any past result  
âœ… Error messages are helpful  
âœ… System works 24/7  
âœ… Performance is < 1 minute per job  

---

## COPILOT COMMAND SUMMARY

### For Quick Implementation:

1. Create server: `Prompt 1 in src/server.js`
2. Create routes: `Prompt 6 in src/routes/`
3. Create validators: `Prompt 2 in src/utils/validators.js`
4. Create processor: `Prompt 3 in src/utils/excelProcessor.js`
5. Create database: `Prompt 4 & 5 in src/db/`
6. Create middleware: `Prompt 7 in src/middleware/`
7. Create config: `Prompt 8 in config/config.js`

---

## TROUBLESHOOTING

### "Port already in use"
- Kill existing process: `lsof -i :3000` then `kill -9 <pid>`
- Or change PORT in .env

### "Cannot connect to database"
- Check DATABASE_URL is correct
- Ensure PostgreSQL is running
- Test with: `psql $DATABASE_URL`

### "File upload fails"
- Check uploads directory exists
- Check disk space available
- Check file permissions
- Check MAX_FILE_SIZE limit

### "Processing times out"
- Check for large files
- Monitor database performance
- Check for memory leaks
- Increase timeout limits

### "Results file incorrect"
- Verify Excel processor formula
- Check merge key logic
- Verify column mapping
- Test with known data

---

**END OF DIRECTIVES**

Use these instructions with GitHub Copilot to implement the complete backend system.
